{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "178fd1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.1\n",
      "0.11.2\n",
      "8.4.0\n"
     ]
    }
   ],
   "source": [
    "from models.ResnetPatchAE import PatchAutoEncoder\n",
    "# from models.ConvPatchAE import PatchAutoEncoder\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torch\n",
    "import math\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)\n",
    "print(Image.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe6287d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchCIFAR100(torchvision.datasets.CIFAR100):\n",
    "    \"\"\"Overrides torchvision CIFAR100 to return patches and class targets\n",
    "    \"\"\"\n",
    "    def __init__(self, transforms=None, grid_size=4, **kwds):\n",
    "        super().__init__(**kwds)\n",
    "        self.transforms = transforms\n",
    "        self.grid_size = grid_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image, class_targets = self.data[index], self.targets[index]\n",
    "\n",
    "        if len(image.shape) == 2:\n",
    "            image = gray2rgb(image)\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image)  \n",
    "\n",
    "        shape = np.array(image.shape)\n",
    "        patch_rw, patch_cl = shape[1]//self.grid_size, shape[2]//self.grid_size\n",
    "\n",
    "        scale = T.Compose([T.Resize((patch_rw*self.grid_size, patch_cl*self.grid_size))])\n",
    "        padding = torch.nn.ZeroPad2d((patch_cl, patch_cl, patch_rw, patch_rw))\n",
    "        img = padding(scale(image))\n",
    "        patches = img.data.unfold(0, 3, 3).unfold(1, patch_rw, patch_rw).unfold(2, patch_cl, patch_cl)\n",
    "\n",
    "        neighbours = torch.zeros(self.grid_size*self.grid_size, 8, shape[0], patch_rw, patch_cl)\n",
    "        target = torch.zeros(self.grid_size*self.grid_size, shape[0], patch_rw, patch_cl)\n",
    "\n",
    "        k = 0\n",
    "\n",
    "        for i in range(1, self.grid_size+1):\n",
    "            for j in range(1, self.grid_size+1):\n",
    "\n",
    "                neighbours[k, 0, :, :, :] = patches[0, i-1, j-1, :, :, :]\n",
    "                neighbours[k, 1, :, :, :] = patches[0, i-1, j, :, :, :]\n",
    "                neighbours[k, 2, :, :, :] = patches[0, i-1, j+1, :, :, :]\n",
    "                neighbours[k, 3, :, :, :] = patches[0, i, j-1, :, :, :]\n",
    "                target[k, :, :, :] = patches[0, i, j, :, :, :]\n",
    "                neighbours[k, 4, :, :, :] = patches[0, i, j+1, :, :, :]\n",
    "                neighbours[k, 5, :, :, :] = patches[0, i+1, j-1, :, :, :]\n",
    "                neighbours[k, 6, :, :, :] = patches[0, i+1, j, :, :, :]\n",
    "                neighbours[k, 7, :, :, :] = patches[0, i+1, j+1, :, :, :]\n",
    "\n",
    "                k += 1           \n",
    "\n",
    "        return target, class_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8620a872",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = T.Compose([\n",
    "                        T.ToPILImage(),\n",
    "                        T.ToTensor(),\n",
    "                      ])\n",
    "batch_size = 128\n",
    "grid_size=4\n",
    "\n",
    "train_set_lineval = PatchCIFAR100(transforms=transforms, \n",
    "                     grid_size=grid_size,\n",
    "                     root='data', train=True, \n",
    "                     )\n",
    "test_set_lineval = PatchCIFAR100(transforms=transforms, \n",
    "                     grid_size=grid_size,\n",
    "                     root='data', train=False, \n",
    "                     )\n",
    "\n",
    "train_loader_lineval = torch.utils.data.DataLoader(train_set_lineval, batch_size=batch_size, shuffle=True)\n",
    "test_loader_lineval = torch.utils.data.DataLoader(test_set_lineval, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88d960ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:  [16, 32, 64]\n",
      "Flatten:  True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backbone_lineval = PatchAutoEncoder(in_channels=3, out_channels=64, flatten=True)\n",
    "cuda = True\n",
    "epoch_num = 120\n",
    "\n",
    "ckp = torch.load('ckpts/cifar_100/resnet_ae_l2_0001_3_augmneted/checkpoint_' + str(epoch_num) + '.ckp', 'cuda' if cuda else None)\n",
    "backbone_lineval.load_state_dict(ckp['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7f38e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_layer = torch.nn.Sequential(torch.nn.Linear(64*16, 100))\n",
    "\n",
    "def backbone_output(model, data):\n",
    "    aggregate = torch.zeros((data.shape[0], 16, 64))\n",
    "    for i in range(data.shape[1]):\n",
    "        output = model(data[:, i])\n",
    "        aggregate[:, i] = output\n",
    "        \n",
    "    return aggregate.reshape((data.shape[0], 16*64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "927d1c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear evaluation\n",
      "Epoch [1] loss: 3.40884; accuracy: 18.32%\n",
      "Epoch [2] loss: 2.88396; accuracy: 30.58%\n",
      "Epoch [3] loss: 2.51456; accuracy: 35.09%\n",
      "Epoch [4] loss: 2.45039; accuracy: 37.98%\n",
      "Epoch [5] loss: 2.50128; accuracy: 40.05%\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(linear_layer.parameters())                               \n",
    "CE = torch.nn.CrossEntropyLoss()\n",
    "linear_layer.train()\n",
    "backbone_lineval.encoder.tower.eval()\n",
    "\n",
    "print('Linear evaluation')\n",
    "for epoch in range(5):\n",
    "    accuracy_list = list()\n",
    "    step = 0\n",
    "    \n",
    "    for i, (data, target) in enumerate(train_loader_lineval):\n",
    "        optimizer.zero_grad()\n",
    "        output = backbone_output(backbone_lineval.encoder.tower, data).detach()        \n",
    "        output = linear_layer(output)\n",
    "        loss = CE(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Estimate the accuracy\n",
    "        prediction = output.argmax(-1)\n",
    "        correct = prediction.eq(target.view_as(prediction)).sum()\n",
    "        accuracy = (100.0 * correct / len(target))\n",
    "        accuracy_list.append(accuracy.item())\n",
    "        \n",
    "    print('Epoch [{}] loss: {:.5f}; accuracy: {:.2f}%' \\\n",
    "            .format(epoch+1, loss.item(), sum(accuracy_list)/len(accuracy_list)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73a62dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 32.91%\n"
     ]
    }
   ],
   "source": [
    "accuracy_list = list()\n",
    "for i, (data, target) in enumerate(test_loader_lineval):\n",
    "    output = backbone_output(backbone_lineval.encoder.tower, data).detach()\n",
    "    output = linear_layer(output)\n",
    "     # Estimate the accuracy\n",
    "    prediction = output.argmax(-1)\n",
    "    correct = prediction.eq(target.view_as(prediction)).sum()\n",
    "    accuracy = (100.0 * correct / len(target))\n",
    "    accuracy_list.append(accuracy.item())\n",
    "\n",
    "print('Test accuracy: {:.2f}%'.format(sum(accuracy_list)/len(accuracy_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b85489",
   "metadata": {},
   "source": [
    "| Model      | Test Accuracy | Epoch|\n",
    "| ---------- | ----------- | ----------- |\n",
    "| ResnetPatchAEInner      | 10.18 %  |  100  |\n",
    "| ResnetPatchAE   |      27.30%   | 100|\n",
    "| ResnetPatchAE   |      30.46%   | 10|\n",
    "| ResnetPatchAEAugmented   |      34.08%   | 10|\n",
    "| ResnetPatchAEAugmented   |      33.18%   | 30|\n",
    "| ResnetAEAugmented   |      16.15%   | 10|\n",
    "| ResnetAEAugmented   |      17.81%   | 100|\n",
    "| Conv4AEAugmented   |      24.55%   | 10|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
